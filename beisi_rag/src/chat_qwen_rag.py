# /Users/profighted/beisi-tech/docs/RAG-Anything/beisi_rag/chat_qwen_rag.py
import os
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage

from openai import OpenAI

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / "config" / ".env")

# æœ¬åœ°å‘é‡åº“ç›®å½•ï¼ˆç¡®ä¿ä¸ ingest é˜¶æ®µä¸€è‡´ï¼‰
INDEX_DIR = Path(__file__).parent.parent / "vectordb"

# ====== é…ç½® Qwen å…¼å®¹ç«¯ç‚¹ ======
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
assert DASHSCOPE_API_KEY, "è¯·å…ˆ export DASHSCOPE_API_KEY=ä½ çš„é€šä¹‰ DashScope Key"

# å¦‚éœ€æ–°åŠ å¡åœ°åŸŸï¼Œæ”¹æˆ https://dashscope-intl.aliyuncs.com/compatible-mode/v1
DASHSCOPE_BASE_URL = os.environ.get(
    "DASHSCOPE_BASE_URL",
    "https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# å¯¹è¯æ¨¡å‹ & å‘é‡æ¨¡å‹ï¼ˆæŒ‰ä½ è´¦å·å¯ç”¨æƒ…å†µè°ƒæ•´ï¼‰
CHAT_MODEL = os.environ.get("QWEN_CHAT_MODEL", "qwen-plus")           # ä¹Ÿå¯ç”¨ qwen2.5-7b-instruct / qwen-turbo ç­‰
EMBEDDING_MODEL = os.environ.get("QWEN_EMBED_MODEL", "text-embedding-v3")


def load_retriever():
    """åŠ è½½ FAISS æ£€ç´¢å™¨ï¼ˆä¸ ingest ä½¿ç”¨åŒä¸€ Embedding æ¨¡å‹ï¼‰"""
    embeddings = DashScopeEmbeddings(
        dashscope_api_key=DASHSCOPE_API_KEY,
        model=EMBEDDING_MODEL,
    )
    vectordb = FAISS.load_local(str(INDEX_DIR), embeddings, allow_dangerous_deserialization=True)
    return vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 4})


def build_llm_runnable():
    """
    ç”¨ OpenAI å…¼å®¹ç«¯ç‚¹ï¼ˆQwenï¼‰æ„å»ºä¸€ä¸ª LangChain Runnableã€‚
    é¿å…ä½¿ç”¨ langchain_openaiï¼Œé™ä½ç‰ˆæœ¬ä¾èµ–å†²çªé£é™©ã€‚
    """
    client = OpenAI(api_key=DASHSCOPE_API_KEY, base_url=DASHSCOPE_BASE_URL)

    def _lc_to_openai_messages(prompt_value) -> list[dict]:
        """
        å°† LangChain çš„ PromptValue / BaseMessage åˆ—è¡¨è½¬æ¢ä¸º OpenAI å…¼å®¹ messagesã€‚
        """
        if hasattr(prompt_value, "to_messages"):
            msgs = prompt_value.to_messages()  # List[BaseMessage]
        elif isinstance(prompt_value, list) and all(isinstance(m, BaseMessage) for m in prompt_value):
            msgs = prompt_value
        else:
            # å…œåº•ï¼šå½“æˆç”¨æˆ·å•è½®è¾“å…¥
            msgs = [HumanMessage(content=str(prompt_value))]

        out = []
        for m in msgs:
            if isinstance(m, SystemMessage):
                role = "system"
            elif isinstance(m, HumanMessage):
                role = "user"
            elif isinstance(m, AIMessage):
                role = "assistant"
            else:
                role = "user"
            out.append({"role": role, "content": m.content})
        return out

    def _invoke(prompt_value: BaseMessage | list[BaseMessage] | str) -> str:
        messages = _lc_to_openai_messages(prompt_value)
        resp = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=messages,
            temperature=0.3,
        )
        return resp.choices[0].message.content

    return RunnableLambda(_invoke)


# RAG_PROMPT = ChatPromptTemplate.from_template(
#     """ä½ æ˜¯ä¸¥è°¨çš„æ£€ç´¢å¢å¼ºåŠ©æ‰‹ã€‚ç»“åˆ<å·²æ£€ç´¢ä¸Šä¸‹æ–‡>å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
# - å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡é‡Œï¼Œè¯·æ˜ç¡®è¯´æ˜â€œä¸ç¡®å®šâ€å¹¶ç»™å‡ºä½ èƒ½ç¡®è®¤çš„çº¿ç´¢ã€‚
# - ç”¨ä¸­æ–‡è¾“å‡ºï¼Œå°½é‡ç»™å‡ºå¼•ç”¨çš„åŸå¥æ‘˜è¦ï¼Œå¹¶åœ¨æœ«å°¾æ ‡æ³¨å¼•ç”¨ç¼–å·ï¼ˆå¦‚ [1][3]ï¼‰ã€‚

# <å·²æ£€ç´¢ä¸Šä¸‹æ–‡>
# {context}
# </å·²æ£€ç´¢ä¸Šä¸‹æ–‡>

# ç”¨æˆ·é—®é¢˜ï¼š{question}
# """
# )

RAG_PROMPT = ChatPromptTemplate.from_template(
    """ä½ æ˜¯ä¸¥è°¨çš„æ£€ç´¢å¢å¼ºåŠ©æ‰‹ã€‚ç»“åˆ<å·²æ£€ç´¢ä¸Šä¸‹æ–‡>å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    -- å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡é‡Œï¼Œè¯·æ˜ç¡®è¯´æ˜â€œä¸ç¡®å®šâ€å¹¶ç»™å‡ºä½ èƒ½ç¡®è®¤çš„çº¿ç´¢ã€‚
    -- ç”¨ä¸­æ–‡è¾“å‡ºï¼Œå°½é‡ç»™å‡ºå¼•ç”¨çš„åŸå¥æ‘˜è¦ï¼Œå¹¶åœ¨æœ«å°¾æ ‡æ³¨å¼•ç”¨ç¼–å·ï¼ˆå¦‚ [1][3]ï¼‰ã€‚
        ä½ æ˜¯ä¸¥è°¨çš„æ£€ç´¢å¢å¼ºåŠ©æ‰‹ã€‚è¯·**ç”¨ä½ è‡ªå·±çš„è¯**ç»¼åˆå›ç­”ï¼Œç¦æ­¢å¤§æ®µåŸæ–‡ç²˜è´´ã€‚
    +è§„åˆ™ï¼š
    +1) å…ˆæ€»ç»“ï¼Œå†ç»™å‡ºå¤„ï¼›ç­”æ¡ˆä¸»ä½“å¿…é¡»æ˜¯**ä½ è‡ªå·±çš„è¡¨è¿°**ã€‚
    +2) å¦‚éœ€å¼•ç”¨åŸå¥ï¼Œæ¯å¤„å¼•ç”¨â‰¤50å­—ï¼Œå¹¶ç”¨å¼•å·ä¸ç¼–å·æ ‡æ³¨ï¼Œå¦‚ â€œâ€¦â€¦â€[1]ã€‚
    +3) å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·è¯´â€œä¸ç¡®å®šâ€ï¼Œå¹¶ç»™å‡ºå¯éªŒè¯çš„çº¿ç´¢ã€‚
    +4) è¾“å‡ºä¸­æ–‡ã€ç»“æ„åŒ–è¦ç‚¹ï¼Œå¹¶åœ¨æœ«å°¾åˆ—å‡ºå‚è€ƒç¼–å·ã€‚
    
    <å·²æ£€ç´¢ä¸Šä¸‹æ–‡>
    {context}
    </å·²æ£€ç´¢ä¸Šä¸‹æ–‡>
    
    ç”¨æˆ·é—®é¢˜ï¼š{question}
    """
)



def format_docs(docs):
    out = []
    for i, d in enumerate(docs, 1):
        meta = d.metadata or {}
        src = meta.get("source", "unknown")
        # æ‘˜è¦æœ€å¤š 500 å­—ç¬¦ï¼Œé¿å…è¿‡é•¿æç¤ºä¸Šä¸‹æ–‡
        out.append(f"[{i}] ({src}) {d.page_content[:500]}")
    return "\n\n".join(out)


def main():
    retriever = load_retriever()
    llm_runnable = build_llm_runnable()

    # RAG é“¾ï¼šæ£€ç´¢ â†’ æ‹¼æ¥ä¸Šä¸‹æ–‡ â†’ æç¤ºè¯ â†’ Qwen(å…¼å®¹ç«¯ç‚¹) â†’ è§£ææ–‡æœ¬
    chain = (
        RunnableParallel(context=retriever | format_docs, question=RunnablePassthrough())
        | RAG_PROMPT
        | llm_runnable
        | StrOutputParser()
    )

    print("ğŸ’¬ è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆCtrl+C é€€å‡ºï¼‰")
    while True:
        try:
            q = input("> ").strip()
            if not q:
                continue
            ans = chain.invoke(q)
            print("\n" + ans + "\n")
        except (EOFError, KeyboardInterrupt):
            print("\nå†è§ï½")
            break
        except Exception as e:
            print("âŒ å‡ºé”™ï¼š", e)


if __name__ == "__main__":
    main()
